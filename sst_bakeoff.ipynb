{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bake-off: Stanford Sentiment Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Bake-off submission](#Bake-off-submission)\n",
    "0. [Methodological note](#Methodological-note)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Baseline](#Baseline)\n",
    "0. [TfRNNClassifier wrapper](#TfRNNClassifier-wrapper)\n",
    "0. [TreeNN wrapper](#TreeNN-wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this in-class bake-off is to __achieve the highest average F1 score__ on the SST development set, with the binary class function.\n",
    "\n",
    "The only restriction: __you cannot make any use of the subtree labels__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off submission\n",
    "\n",
    "1. A description of the model you created.\n",
    "1. The value of `f1-score` in the `avg / total` row of the classification report.\n",
    "\n",
    "Submission URL: https://docs.google.com/forms/d/1R41Zxxils7lOPzuThMdv2p1TKmFEy8c0DyUg-YkzTa0/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodological note\n",
    "\n",
    "You don't have to use the experimental framework defined below (based on `sst`). However, if you don't use `sst.experiment` as below, then make sure you're training only on `train`, evaluating on `dev`, and that you report with \n",
    "\n",
    "```\n",
    "from sklearn.metrics import classification_report\n",
    "classification_report(y_dev, predictions)\n",
    "```\n",
    "where `y_dev = [y for tree, y in sst.dev_reader(class_func=sst.binary_class_func)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](sst_01_overview.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from rnn_classifier import RNNClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import sst\n",
    "import tensorflow as tf\n",
    "from tf_rnn_classifier import TfRNNClassifier\n",
    "from tree_nn import TreeNN\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(tree):\n",
    "    \"\"\"The basis for a unigrams feature function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : nltk.tree\n",
    "        The tree to represent.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    defaultdict\n",
    "        A map from strings to their counts in `tree`. (Counter maps a \n",
    "        list to a dict of counts of the elements in that list.)\n",
    "    \n",
    "    \"\"\"\n",
    "    return Counter(tree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):\n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 16282)\n",
      "Accuracy: 0.772\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.783     0.741     0.761       428\n",
      "   positive      0.762     0.802     0.782       444\n",
      "\n",
      "avg / total      0.772     0.772     0.772       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    unigrams_phi,                      # Free to write your own!\n",
    "    fit_maxent_classifier,             # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed.\n",
    "    class_func=sst.binary_class_func)  # Fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, with some informal hyperparameter search on a GPU machine, I found this model\n",
    "```\n",
    "tf_rnn_glove = TfRNNClassifier(\n",
    "    sst_glove_vocab,\n",
    "    embedding=glove_embedding, ## 100d version\n",
    "    hidden_dim=300,\n",
    "    max_length=52,\n",
    "    hidden_activation=tf.nn.relu,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=True,\n",
    "    max_iter=5000,\n",
    "    batch_size=1028,\n",
    "    eta=0.001)\n",
    "```\n",
    "which finished with almost identical performance to the above:\n",
    "    \n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "   negative       0.78      0.75      0.76       428\n",
    "   positive       0.77      0.80      0.78       444\n",
    "\n",
    "avg / total       0.77      0.77      0.77       872\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfRNNClassifier wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(tree):\n",
    "    return tree.leaves()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tf_rnn_classifier(X, y):\n",
    "    vocab = sst.get_vocab(X, n_words=3000)\n",
    "    mod = TfRNNClassifier(\n",
    "        vocab, \n",
    "        eta=0.05,\n",
    "        batch_size=2048,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_length=52, \n",
    "        max_iter=500,\n",
    "        cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "        hidden_activation=tf.nn.tanh,\n",
    "        train_embedding=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 500: loss: 2.5067093968391423"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.654\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.683     0.549     0.609       428\n",
      "   positive      0.634     0.755     0.689       444\n",
      "\n",
      "avg / total      0.658     0.654     0.650       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    rnn_phi,\n",
    "    fit_tf_rnn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreeNN wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_phi(tree):\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree_nn_classifier(X, y):\n",
    "    vocab = sst.get_vocab(X, n_words=3000)\n",
    "    mod = TreeNN(\n",
    "        vocab, \n",
    "        embed_dim=100, \n",
    "        max_iter=100)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.8351342778738807"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.510\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.501     0.498     0.499       428\n",
      "   positive      0.519     0.523     0.521       444\n",
      "\n",
      "avg / total      0.510     0.510     0.510       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    rnn_phi,\n",
    "    fit_tree_nn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni + Bi-Gram Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = sst.build_binary_rnn_dataset(sst.train_reader)\n",
    "sst_train_vocab = sst.get_vocab(X_train, n_words=None)\n",
    "vocab_dict = dict(zip(sst_train_vocab, range(len(sst_train_vocab))))\n",
    "X_dev, y_dev = sst.build_binary_rnn_dataset(sst.dev_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(lists_of_words, vocab_dict):\n",
    "    X = np.zeros((len(lists_of_words), len(vocab_dict)))\n",
    "    for idx, words in enumerate(lists_of_words):\n",
    "        for w in words:\n",
    "            if w in vocab_dict:\n",
    "                X[idx, vocab_dict[w]] += 1\n",
    "            else:\n",
    "                X[idx, vocab_dict['$UNK']] += 1\n",
    "    return X\n",
    "\n",
    "\n",
    "X_train = build_matrix(X_train, vocab_dict)\n",
    "X_dev = build_matrix(X_dev, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3050           20.97m\n",
      "         2           1.2673           20.57m\n",
      "         3           1.2272           20.76m\n",
      "         4           1.1978           21.25m\n",
      "         5           1.1642           21.97m\n",
      "         6           1.1418           21.96m\n",
      "         7           1.1180           21.88m\n",
      "         8           1.1003           22.48m\n",
      "         9           1.0863           22.80m\n",
      "        10           1.0689           22.82m\n",
      "        20           0.9375           21.77m\n",
      "        30           0.8405           18.18m\n",
      "        40           0.7751           15.80m\n",
      "        50           0.7076           13.12m\n",
      "        60           0.6573           10.47m\n",
      "        70           0.6156            7.83m\n",
      "        80           0.5868            5.21m\n",
      "        90           0.5557            2.60m\n",
      "       100           0.5298            0.00s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.74      0.61      0.67       428\n",
      "   positive       0.68      0.79      0.73       444\n",
      "\n",
      "avg / total       0.71      0.70      0.70       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# mod = LogisticRegression()\n",
    "mod = GradientBoostingClassifier(learning_rate=1, max_depth=5, verbose=True)\n",
    "mod.fit(X_train, y_train)\n",
    "print(classification_report(y_dev, mod.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3702           19.28m\n",
      "         2           1.3584           19.85m\n",
      "         3           1.3484           19.94m\n",
      "         4           1.3395           20.12m\n",
      "         5           1.3306           20.23m\n",
      "         6           1.3223           19.43m\n",
      "         7           1.3157           19.19m\n",
      "         8           1.3097           19.18m\n",
      "         9           1.3024           18.73m\n",
      "        10           1.2972           18.58m\n",
      "        20           1.2516           18.65m\n",
      "        30           1.2148           15.95m\n",
      "        40           1.1857           13.17m\n",
      "        50           1.1590           10.62m\n",
      "        60           1.1384            8.34m\n",
      "        70           1.1166            6.12m\n",
      "        80           1.0964            4.07m\n",
      "        90           1.0788            2.04m\n",
      "       100           1.0636            0.00s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.75      0.54      0.63       428\n",
      "   positive       0.65      0.83      0.73       444\n",
      "\n",
      "avg / total       0.70      0.69      0.68       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mod = LogisticRegression()\n",
    "mod = GradientBoostingClassifier(max_depth=5, verbose=True)\n",
    "mod.fit(X_train, y_train)\n",
    "print(classification_report(y_dev, mod.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3545            8.25m\n",
      "         2           1.3302            8.07m\n",
      "         3           1.3097            7.64m\n",
      "         4           1.2927            7.37m\n",
      "         5           1.2785            7.37m\n",
      "         6           1.2661            7.33m\n",
      "         7           1.2554            7.21m\n",
      "         8           1.2464            7.14m\n",
      "         9           1.2383            7.12m\n",
      "        10           1.2317            7.06m\n",
      "        20           1.1895            6.68m\n",
      "        30           1.1625            5.95m\n",
      "        40           1.1421            5.13m\n",
      "        50           1.1240            4.27m\n",
      "        60           1.1076            3.37m\n",
      "        70           1.0945            2.50m\n",
      "        80           1.0812            1.67m\n",
      "        90           1.0688           49.92s\n",
      "       100           1.0588            0.00s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.54      0.79      0.64       428\n",
      "   positive       0.64      0.36      0.46       444\n",
      "\n",
      "avg / total       0.59      0.57      0.55       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mod = LogisticRegression()\n",
    "mod = GradientBoostingClassifier(verbose=True)\n",
    "mod.fit(X_train, y_train)\n",
    "print(classification_report(y_dev, mod.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 89960 features per sample; expecting 3000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/workspace/cs224u/sst.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(phi, train_func, train_reader, assess_reader, train_size, class_func, score_func, vectorize, verbose)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;31m# Predictions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_assess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;31m# Report:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/cs224u/venv/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/cs224u/venv/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 89960 features per sample; expecting 3000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def fit_gdbt_classifier(X, y):\n",
    "    #dim reduction by taking\n",
    "    X = X[:, np.array(X.sum(axis=0)).ravel().argsort()[-3000:][::-1]]\n",
    "    mod = LogisticRegression()\n",
    "#     mod = GradientBoostingClassifier()\n",
    "    mod.fit(X, y)\n",
    "    return mod\n",
    "\n",
    "def uni_bi_gram_phi(tree):\n",
    "    leaves = tree.leaves()\n",
    "    d = Counter(['<S>' + val + ' ' + leaves[idx+1] + '</S>' for idx, val in enumerate(leaves) if idx != (len(leaves)-1)])\n",
    "    d.update(Counter(leaves))\n",
    "    return d\n",
    "\n",
    "_ = sst.experiment(\n",
    "    uni_bi_gram_phi,            \n",
    "    fit_gdbt_classifier,\n",
    "    train_reader=sst.train_reader,\n",
    "    assess_reader=sst.dev_reader,\n",
    "    class_func=sst.binary_class_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.786     0.745     0.765       428\n",
      "   positive      0.766     0.804     0.785       444\n",
      "\n",
      "avg / total      0.776     0.775     0.775       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def uni_bi_gram_phi(tree):\n",
    "    leaves = tree.leaves()\n",
    "    d = Counter(['<S>' + val + ' ' + leaves[idx+1] + '</S>' for idx, val in enumerate(leaves) if idx != (len(leaves)-1)])\n",
    "    d.update(Counter(leaves))\n",
    "    return d\n",
    "\n",
    "_ = sst.experiment(\n",
    "    uni_bi_gram_phi,            \n",
    "    fit_maxent_classifier,\n",
    "    train_reader=sst.train_reader,\n",
    "    assess_reader=sst.dev_reader,\n",
    "    class_func=sst.binary_class_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfBidirectionalRNNClassifier(TfRNNClassifier):\n",
    "    \n",
    "    def build_graph(self):\n",
    "        self._define_embedding()\n",
    "\n",
    "        self.inputs = tf.placeholder(\n",
    "            tf.int32, [None, self.max_length])\n",
    "\n",
    "        self.ex_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        # Outputs as usual:\n",
    "        self.outputs = tf.placeholder(\n",
    "            tf.float32, shape=[None, self.output_dim])\n",
    "\n",
    "        # This converts the inputs to a list of lists of dense vector\n",
    "        # representations:\n",
    "        self.feats = tf.nn.embedding_lookup(\n",
    "            self.embedding, self.inputs)\n",
    "\n",
    "        # Same cell structure as the base class, but we have\n",
    "        # forward and backward versions:\n",
    "        self.cell_fw = tf.nn.rnn_cell.LSTMCell(\n",
    "            self.hidden_dim, activation=self.hidden_activation)\n",
    "        \n",
    "        self.cell_bw = tf.nn.rnn_cell.LSTMCell(\n",
    "            self.hidden_dim, activation=self.hidden_activation)\n",
    "\n",
    "        # Run the RNN:\n",
    "        outputs, finals = tf.nn.bidirectional_dynamic_rnn(\n",
    "            self.cell_fw,\n",
    "            self.cell_bw,\n",
    "            self.feats,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.ex_lengths)\n",
    "      \n",
    "        # finals is a pair of `LSTMStateTuple` objects, which are themselves\n",
    "        # pairs of Tensors (x, y), where y is the output state, according to\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple\n",
    "        # Thus, we want the second member of these pairs:\n",
    "        last_fw, last_bw = finals          \n",
    "        last_fw, last_bw = last_fw[1], last_bw[1]\n",
    "        \n",
    "        last = tf.concat((last_fw, last_bw), axis=1)\n",
    "        \n",
    "        self.feat_dim = self.hidden_dim * 2               \n",
    "\n",
    "        # Softmax classifier on the final hidden state:\n",
    "        self.W_hy = self.weight_init(\n",
    "            self.feat_dim, self.output_dim, 'W_hy')\n",
    "        self.b_y = self.bias_init(self.output_dim, 'b_y')\n",
    "        self.model = tf.matmul(last, self.W_hy) + self.b_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tf_bidirectional_rnn_classifier(X, y):\n",
    "    vocab = sst.get_vocab(X, n_words=3000)\n",
    "    mod = TfBidirectionalRNNClassifier(\n",
    "        vocab, \n",
    "        eta=0.05,\n",
    "        batch_size=2048,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_length=52, \n",
    "        max_iter=500,\n",
    "        cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "        hidden_activation=tf.nn.tanh,\n",
    "        train_embedding=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod\n",
    "\n",
    "_ = sst.experiment(\n",
    "    rnn_phi,\n",
    "    fit_tf_bidirectional_rnn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 500: loss: 2.4356548786163334"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.653\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.646     0.647     0.646       428\n",
      "   positive      0.659     0.658     0.658       444\n",
      "\n",
      "avg / total      0.653     0.653     0.653       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit_tf_bidirectional_rnn_classifier(X, y):\n",
    "    vocab = sst.get_vocab(X, n_words=3000)\n",
    "    mod = TfBidirectionalRNNClassifier(\n",
    "        vocab, \n",
    "        eta=0.05,\n",
    "        batch_size=2048,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_length=52, \n",
    "        max_iter=500,\n",
    "        cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "        hidden_activation=tf.nn.tanh,\n",
    "        train_embedding=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod\n",
    "\n",
    "_ = sst.experiment(\n",
    "    rnn_phi,\n",
    "    fit_tf_bidirectional_rnn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 500: loss: 2.4546531438827515"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.636\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative      0.636     0.607     0.621       428\n",
      "   positive      0.637     0.664     0.650       444\n",
      "\n",
      "avg / total      0.636     0.636     0.636       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit_tf_bidirectional_rnn_classifier_relu(X, y):\n",
    "    vocab = sst.get_vocab(X, n_words=3000)\n",
    "    mod = TfBidirectionalRNNClassifier(\n",
    "        vocab, \n",
    "        eta=0.05,\n",
    "        batch_size=2048,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_length=52, \n",
    "        max_iter=500,\n",
    "        cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "        hidden_activation=tf.nn.relu,\n",
    "        train_embedding=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod\n",
    "\n",
    "_ = sst.experiment(\n",
    "    rnn_phi,\n",
    "    fit_tf_bidirectional_rnn_classifier_relu, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
